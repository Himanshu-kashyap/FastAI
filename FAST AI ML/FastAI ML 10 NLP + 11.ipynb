{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FastAI ML 10 NLP.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/fastai/fastai/blob/master/courses/ml1/lesson5-nlp.ipynb","timestamp":1544523112828}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"QJfAghNf4IEg","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install \"fastai==0.7.0\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"2GtVx1bJwYB3","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install Pillow==4.1.1\n","\n","!pip install torchtext==0.2.3\n","!apt-get install gunzip\n","!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!gunzip aclImdb_v1.tar.gz\n","!tar -xvf aclImdb_v1.tar\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4MnG0iVnS4io","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip list"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0of-DtXA3KS8","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"l8x09rATwECo","colab_type":"code","colab":{}},"cell_type":"code","source":["%reload_ext autoreload\n","%autoreload 2\n","%matplotlib inline\n","\n","from fastai.nlp import *\n","from sklearn.linear_model import LogisticRegression"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mMHU8DrpwEC3","colab_type":"text"},"cell_type":"markdown","source":["## IMDB dataset and the sentiment classification task"]},{"metadata":{"id":"zbl4NeYJwEC6","colab_type":"text"},"cell_type":"markdown","source":["The [large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) contains a collection of 50,000 reviews from IMDB. The dataset contains an even number of positive and negative reviews. The authors considered only highly polarized reviews. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. Neutral reviews are not included in the dataset. The dataset is divided into training and test sets. The training set is the same 25,000 labeled reviews.\n","\n","The **sentiment classification task** consists of predicting the polarity (positive or negative) of a given text.\n","\n","To get the dataset, in your terminal run the following commands:\n","\n","`wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz`\n","\n","`gunzip aclImdb_v1.tar.gz`\n","\n","`tar -xvf aclImdb_v1.tar`"]},{"metadata":{"id":"8aKQlvZowEC9","colab_type":"text"},"cell_type":"markdown","source":["### Tokenizing and term document matrix creation"]},{"metadata":{"id":"0vM8ABFEwEDC","colab_type":"code","colab":{}},"cell_type":"code","source":["PATH='aclImdb/'\n","names = ['neg','pos']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1AS6_ICDwEDM","colab_type":"code","outputId":"b6367d93-5ab8-4b49-cacc-1bfa01c8ff97","executionInfo":{"status":"ok","timestamp":1545112645795,"user_tz":-330,"elapsed":206707,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["%ls {PATH}"],"execution_count":6,"outputs":[{"output_type":"stream","text":["imdbEr.txt  imdb.vocab  README  \u001b[0m\u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/\n"],"name":"stdout"}]},{"metadata":{"id":"Mnfx_kaYwEDY","colab_type":"code","outputId":"a6bcdbfb-570f-4989-f3e0-10f47333655f","executionInfo":{"status":"ok","timestamp":1545114778816,"user_tz":-330,"elapsed":6400,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["%ls {PATH}train\n","# positive and negative reviews"],"execution_count":7,"outputs":[{"output_type":"stream","text":["labeledBow.feat  \u001b[0m\u001b[01;34mpos\u001b[0m/    unsupBow.feat  urls_pos.txt\n","\u001b[01;34mneg\u001b[0m/             \u001b[01;34munsup\u001b[0m/  urls_neg.txt   urls_unsup.txt\n"],"name":"stdout"}]},{"metadata":{"id":"Rl2hi4tmwEDg","colab_type":"code","outputId":"5740de26-fe6c-4e4a-fcae-82d1528a19dd","executionInfo":{"status":"ok","timestamp":1545114785022,"user_tz":-330,"elapsed":5760,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"cell_type":"code","source":["%ls {PATH}train/pos | head"],"execution_count":8,"outputs":[{"output_type":"stream","text":["0_9.txt\n","10000_8.txt\n","10001_10.txt\n","10002_7.txt\n","10003_8.txt\n","10004_8.txt\n","10005_7.txt\n","10006_7.txt\n","10007_7.txt\n","10008_7.txt\n"],"name":"stdout"}]},{"metadata":{"id":"e2LUTJ7FwEDr","colab_type":"code","colab":{}},"cell_type":"code","source":["trn,trn_y = texts_labels_from_folders(f'{PATH}train',names) # find out all the values from these folders arg1 with names[neg and pos]\n","val,val_y = texts_labels_from_folders(f'{PATH}test',names)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WgUnipj5CFOD","colab_type":"code","colab":{}},"cell_type":"code","source":["??texts_labels_from_folders"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1IfDmVAGwED2","colab_type":"text"},"cell_type":"markdown","source":["Here is the text of the first review"]},{"metadata":{"id":"48A8CIlywED9","colab_type":"code","outputId":"953fe0d7-fc60-418e-aee3-cbca8988d26c","executionInfo":{"status":"ok","timestamp":1545114807869,"user_tz":-330,"elapsed":1094,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"cell_type":"code","source":["trn[0]\n"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Are you familiar with concept of children\\'s artwork? While it is not the greatest Picasso any three-year-old has ever accomplished with their fingers, you encourage them to do more. If painting is what makes them happy, there should be no reason a parent should hold that back on a child. Typically, if a child loves to paint or draw, you will immediately see the groundwork of their future style. You will begin to see their true form in these very primitive doodles. Well, this concept of children\\'s artwork is how I felt about Fuqua\\'s depressingly cheap and uncreative film Bait. While on all accounts it was a horrid film, it was impressive to see Fuqua\\'s style begin emerging through even the messiest of moments. If you have seen either Training Day or King Arthur, you will be impressed with the birth of this director in his second film Bait. While Foxx gives a horrid, unchained performance, there are certain scenes, which define Fuqua and demonstrate his brilliance behind the camera. Sadly it only emerged in the final thirty minutes of the film, but if you focus just on those scenes, you will see why Fuqua\\'s name appears on so many \"Best Of\\x85\" film lists.<br /><br />I will never disagree with someone that Fuqua\\'s eye behind the camera is refreshing and unique. His ability to place a camera in the strangest of places to convey the simplest of emotions is shocking. I am surprised that more of Hollywood hasn\\'t jumped aboard this bandwagon. Even in the silly feature Bait, you are witness to Fuqua\\'s greatness. Two scenes that come directly to mind are the explosion scene near the middle of the film and the horse scene close to the end. In both of these scenes I saw the director Fuqua at work. Alas, in the rest of this film, all I saw was a combination of nearly every action film created. The likable hero down on his luck that suddenly finds his life turned around by some unknown force is a classic structure that just needs to die in Hollywood. We have seen this two often, and no matter who you are (unless you are Charlie Kaufmann), you cannot recreate the wheel. It is just impossible with this genre, and it is proved with Bait. I was annoyed with Fuqua for just sitting back and allowing this to happen, which could explain why it took me three viewings to finish this film. I was just tired of the structure, and while I hoped that Fuqua would redefine it, he did not.<br /><br />Then, there was the acting. While Jamie Foxx has never impressed me as an actor, I was willing to give this helmed vehicle a try. I wanted to see if he could pull off another dramatic role similar to Collateral. I was under the impression that perhaps this was the film chosen to show producers that Foxx could handle the role in Collateral. Again, I was disappointed. Foxx was annoying. Not in the sense that it was the way that his character was to be, but in the sense that it felt as if neither Fuqua nor Foxx took the time to fully train Foxx on what should be ad-libed and what should be used to further the plot. Instead, we are downtrodden with scene over scene of Foxx just trying to make the audience laugh. Adding second long quips and culture statements just to keep his audience understanding that he was a comedian first, an actor second. Fuqua should have stopped this immediately. Foxx\\'s jokes destroyed his character, which in turn left me with nothing solid to grasp ahold of. Instead of character development, he would crack a joke. Neither style worked, no joke was funny. The rest of the cast was average. By this I mean I have seen them all in similar roles. They were brining nothing new to the table, nothing solid to the story, and nothing substantial to the overall themes of the film. They were pawns filling in dead air space. Fuqua had no control over this mess, and the final verdict only supports that accusation.<br /><br />Overall, this was a sad film. With no creativity in sight and unmanaged actors just trying to upstage themselves, what originally started as a decent story eventually sunk faster into the cinematic quicksand. Foxx was annoying, without character lines, and a complete bag of cheese. In each scene I saw no emotion, and when emotion was needed to convey a message, he chose to take his shirt off rather than tackle the issues. Are my words harsh? I don\\'t think so. When you watch any movie you want to see some creativity, some edible characters, and themes that seem to hit close to home. Bait contained none of these. While I will give Fuqua some credit for two of the scenes in this film, the remaining five hundred were disastrous. Apparently, I took the bait when renting this film, but now having seen it, hopefully I can stop others from taking that curious nibble.<br /><br />Grade: ** out of ***** (for his two scenes that were fun to watch)'"]},"metadata":{"tags":[]},"execution_count":10}]},{"metadata":{"id":"lXSil2cewEEU","colab_type":"code","outputId":"3ccb252a-99e6-4d7f-bb91-3645143f4450","executionInfo":{"status":"ok","timestamp":1544615666062,"user_tz":-330,"elapsed":217018,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["trn_y[0]  #0 is negative review"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":14}]},{"metadata":{"id":"ulqmgogfwEEf","colab_type":"text"},"cell_type":"markdown","source":["[`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) converts a collection of text documents to a matrix of token counts (part of `sklearn.feature_extraction.text`)."]},{"metadata":{"id":"M3XdeurkwEEh","colab_type":"code","colab":{}},"cell_type":"code","source":["# creating a token\n","veczr = CountVectorizer(tokenizer=tokenize)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SK3FovrfF6kM","colab_type":"code","colab":{}},"cell_type":"code","source":["??CountVectorizer"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6A7PMgDVwEEo","colab_type":"text"},"cell_type":"markdown","source":["`fit_transform(trn)` finds the vocabulary in the training set. It also transforms the training set into a term-document matrix. Since we have to apply the *same transformation* to your validation set, the second line uses just the method `transform(val)`. `trn_term_doc` and `val_term_doc` are sparse matrices. `trn_term_doc[i]` represents training document i and it contains a count of words for each document for each word in the vocabulary."]},{"metadata":{"id":"A82Xe6aQwEEr","colab_type":"code","colab":{}},"cell_type":"code","source":["trn_term_doc = veczr.fit_transform(trn) # create vocab and create term doc matrix based on trn set\n","val_term_doc = veczr.transform(val) # use previously fitted model / vocab for valid set\n","\n","# if a new word arrives in validation set tokenizer makes another field named unknown"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wTWmoeef_suX","colab_type":"code","colab":{}},"cell_type":"code","source":["// ??veczr.fit_transform"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qWFAOglTwEE0","colab_type":"code","outputId":"de14b62f-d36f-4d77-a73d-4ff901ac631d","executionInfo":{"status":"ok","timestamp":1545116725656,"user_tz":-330,"elapsed":764,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["trn_term_doc\n","# do NOT store as an array\n","# stored in way of only showing the no. of times the occurence happened\n","# e.g. doc 1 word no 12 came in 6 times so sparse matrix will save as\n","# (1,12) -> 6"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<25000x75132 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 3749745 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":16}]},{"metadata":{"id":"d1XCJO2swEE-","colab_type":"code","outputId":"7c0f263d-126a-487d-d29b-7acbcc7ba937","executionInfo":{"status":"ok","timestamp":1545116753002,"user_tz":-330,"elapsed":993,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["trn_term_doc[0]  # 403 words of total 75132 are in doc 0"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<1x75132 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 403 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":17}]},{"metadata":{"id":"ZHkWLBHzwEFE","colab_type":"code","outputId":"6e24f39c-f346-4677-e151-ae16b52d3ead","executionInfo":{"status":"ok","timestamp":1545116811189,"user_tz":-330,"elapsed":1327,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# get feature names maps integers to words\n","vocab = veczr.get_feature_names(); vocab[5000:5005]"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['aussie', 'aussies', 'austen', 'austeniana', 'austens']"]},"metadata":{"tags":[]},"execution_count":18}]},{"metadata":{"id":"706rl6zXRwJ0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6b5e3595-272a-474b-e1f5-441a4bdbf74f","executionInfo":{"status":"ok","timestamp":1545119147582,"user_tz":-330,"elapsed":1128,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["vocab = veczr.get_feature_names(); vocab[0:5]"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\\x08\\x08\\x08\\x08a', '\\x10own', '!', '\"', '#']"]},"metadata":{"tags":[]},"execution_count":22}]},{"metadata":{"id":"39kSRiHdwEFP","colab_type":"code","colab":{}},"cell_type":"code","source":["# splitting based on space and not using real tokenizer and converting into \n","# lower case just to see order of appearence \n","\n","w0 = set([o.lower() for o in trn[0].split(' ')]); w0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7wZ9p8WZwEFh","colab_type":"code","outputId":"039b5253-6b98-4b51-c430-6ef835367c51","executionInfo":{"status":"ok","timestamp":1544615680007,"user_tz":-330,"elapsed":230816,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["len(w0) # total number of words appearing "],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["108"]},"metadata":{"tags":[]},"execution_count":22}]},{"metadata":{"id":"267j5DFMwEFw","colab_type":"code","outputId":"45d35c13-eff2-4568-99c5-215712216dfa","executionInfo":{"status":"ok","timestamp":1545117419805,"user_tz":-330,"elapsed":992,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# maps words to integers\n","veczr.vocabulary_['could'] \n","\n","# this method returns index of the word and kindof is \n","# opposite to get_feature_names"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15042"]},"metadata":{"tags":[]},"execution_count":19}]},{"metadata":{"id":"cSzxv2PFwEF7","colab_type":"code","outputId":"11c1ac3f-fd10-48bc-efec-b2020da550c4","executionInfo":{"status":"ok","timestamp":1545117468609,"user_tz":-330,"elapsed":1003,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["trn_term_doc[0,15042] # find value 'could' in the vocab \n","# verified from sublime"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{"tags":[]},"execution_count":20}]},{"metadata":{"id":"QbDepNp4wEGG","colab_type":"code","outputId":"dab29b06-9b11-4c8e-cf57-aef226274bf8","executionInfo":{"status":"ok","timestamp":1545119111026,"user_tz":-330,"elapsed":1123,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["trn_term_doc[0,5000] # find aussie in vocab"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":21}]},{"metadata":{"id":"afibCAhXwEGP","colab_type":"text"},"cell_type":"markdown","source":["## Naive Bayes"]},{"metadata":{"id":"VFYYRQSxwEGR","colab_type":"text"},"cell_type":"markdown","source":["We define the **log-count ratio** $r$ for each word $f$:\n","\n","$r = \\log \\frac{\\text{ratio of feature $f$ in positive documents}}{\\text{ratio of feature $f$ in negative documents}}$\n","\n","where ratio of feature $f$ in positive documents is the number of times a positive document has a feature divided by the number of positive documents."]},{"metadata":{"id":"d3ep2cHORq7Z","colab_type":"code","colab":{}},"cell_type":"code","source":["x = trn_term_doc\n","y = trn_y\n","\n","p = x[y==1].sum(0)+1 # numpy adding additional 1 grab row when dep var = 1 and sum to get total word count over rows\n","q = x[y==0].sum(0)+1\n","r = np.log((p/p.sum())/(q/q.sum()))  # taking log so we dont have to mult ; add instead ; ratio of +Ve / -Ve\n","b = np.log(len(p)/len(q))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6fS-5xonwEGS","colab_type":"code","colab":{}},"cell_type":"code","source":["def pr(y_i):\n","    p = x[y==y_i].sum(0)\n","    return (p+1) / ((y==y_i).sum()+1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"n7PJfkJowEGY","colab_type":"code","colab":{}},"cell_type":"code","source":["x=trn_term_doc\n","y=trn_y\n","\n","r = np.log(pr(1)/pr(0))\n","b = np.log((y==1).mean() / (y==0).mean())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dJ_rNk89wEGe","colab_type":"text"},"cell_type":"markdown","source":["Here is the formula for Naive Bayes."]},{"metadata":{"id":"GvIxi0bCwEGj","colab_type":"code","outputId":"586943e4-b6e8-45e5-b9ca-6f2fc22615c5","executionInfo":{"status":"ok","timestamp":1545120243035,"user_tz":-330,"elapsed":1066,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# instead of counting number of occurences we just take presence is or is not\n","# multiply bayes prob by account\n","pre_preds = val_term_doc @ r.T + b #binarized means occurrence will be counted as 1 and negative occ = -1\n","preds = pre_preds.T>0\n","(preds==val_y).mean()"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8074"]},"metadata":{"tags":[]},"execution_count":27}]},{"metadata":{"id":"axiFwDgQwEGv","colab_type":"text"},"cell_type":"markdown","source":["...and binarized Naive Bayes."]},{"metadata":{"id":"WQqENpc1wEGy","colab_type":"code","outputId":"bc76242e-d2f6-4fef-abc3-bec38e6937b1","executionInfo":{"status":"ok","timestamp":1545120638164,"user_tz":-330,"elapsed":1620,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# .sign will just check occurence coming or not instead of number of occurences\n","x=trn_term_doc.sign()\n","# r = np.log(pr(1)/pr(0))\n","\n","pre_preds = val_term_doc.sign() @ r.T + b\n","preds = pre_preds.T>0\n","(preds==val_y).mean()"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.82624"]},"metadata":{"tags":[]},"execution_count":29}]},{"metadata":{"id":"7OjH1pkEwEG3","colab_type":"text"},"cell_type":"markdown","source":["### Logistic regression"]},{"metadata":{"id":"OnYWGM9awEG5","colab_type":"text"},"cell_type":"markdown","source":["Here is how we can fit logistic regression where the features are the unigrams."]},{"metadata":{"id":"CXR_1qeIwEG6","colab_type":"code","outputId":"89a5e93c-9d35-4762-e9bd-f53a09f41fb0","executionInfo":{"status":"ok","timestamp":1545120843690,"user_tz":-330,"elapsed":5265,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["'''\n","Instead of using coefficients(ratio) r we will learn them from data using\n","logistic regression\n","dual = true will reduce computation time when data is wider instead of longer\n","i.e. more colms \n","\n","C = smaller implies more regularization but using as small as 1e8 will turn it off\n","\n","using C = 0.1 == 0.848\n","        = 1e8 == 0.8327\n","\n","'''\n","\n","m = LogisticRegression(C=1e8, dual=True)\n","m.fit(x, y)\n","preds = m.predict(val_term_doc)\n","(preds==val_y).mean()"],"execution_count":34,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["0.83276"]},"metadata":{"tags":[]},"execution_count":34}]},{"metadata":{"id":"jSHcjbFLwEG_","colab_type":"code","outputId":"a64e516f-f2ac-4881-bd6f-17153d8a79cf","executionInfo":{"status":"ok","timestamp":1545120680353,"user_tz":-330,"elapsed":5699,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["''' This is the Binarized version of the fitting '''\n","\n","m = LogisticRegression(C=1e8, dual=True)\n","m.fit(trn_term_doc.sign(), y)\n","preds = m.predict(val_term_doc.sign())\n","(preds==val_y).mean()"],"execution_count":31,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["0.85496"]},"metadata":{"tags":[]},"execution_count":31}]},{"metadata":{"id":"D70sZY9BwEHF","colab_type":"text"},"cell_type":"markdown","source":["...and the regularized version"]},{"metadata":{"id":"VhbPPvWowEHH","colab_type":"code","outputId":"a1eae8ed-b8c7-4769-b9ff-5c08bbd78c8f","executionInfo":{"status":"ok","timestamp":1544615691079,"user_tz":-330,"elapsed":241754,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["# turning on reguralization ; overfitting so adding l2 regularization for overfitting\n","# L2 will not try to make things 0 but if 2 things are corelated then will turn both down\n","# 1 with 0 and 1 not 0\n","# whereas l1 will try to make both 0\n","m = LogisticRegression(C=0.1, dual=True)\n","m.fit(x, y)\n","preds = m.predict(val_term_doc)\n","(preds==val_y).mean()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["0.84872"]},"metadata":{"tags":[]},"execution_count":32}]},{"metadata":{"id":"YerutoPzwEHS","colab_type":"code","outputId":"4a2e428f-727c-40da-e280-36deb5643d11","executionInfo":{"status":"ok","timestamp":1544615692469,"user_tz":-330,"elapsed":243129,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["m = LogisticRegression(C=0.1, dual=True)\n","m.fit(trn_term_doc.sign(), y)\n","preds = m.predict(val_term_doc.sign())\n","(preds==val_y).mean()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["0.88404"]},"metadata":{"tags":[]},"execution_count":33}]},{"metadata":{"id":"mPOjssjpwEHW","colab_type":"text"},"cell_type":"markdown","source":["### Trigram with NB features"]},{"metadata":{"id":"e-Vb8pG3wEHX","colab_type":"text"},"cell_type":"markdown","source":["Our next model is a version of logistic regression with Naive Bayes features described [here](https://www.aclweb.org/anthology/P12-2018). For every document we compute binarized features as described above, but this time we use bigrams and trigrams too. Each feature is a log-count ratio. A logistic regression model is then trained to predict sentiment."]},{"metadata":{"id":"W2B322ZWwEHY","colab_type":"code","colab":{}},"cell_type":"code","source":["#ngrams use upto n words instead of just 1 in range (1,n)\n","# countvectorizer will sort vocab by occurence and cut after 800k words\n","\n","veczr =  CountVectorizer(ngram_range=(1,3), tokenizer=tokenize, max_features=800000)\n","trn_term_doc = veczr.fit_transform(trn)\n","\n","val_term_doc = veczr.transform(val)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1KfxgjrgwEHb","colab_type":"code","outputId":"9ec5a7dd-b6aa-4874-91d3-09c5d395face","executionInfo":{"status":"ok","timestamp":1544615771604,"user_tz":-330,"elapsed":322236,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["trn_term_doc.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(25000, 800000)"]},"metadata":{"tags":[]},"execution_count":35}]},{"metadata":{"id":"7Qvg8wohwEHp","colab_type":"code","colab":{}},"cell_type":"code","source":["vocab = veczr.get_feature_names()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UT1jhVJiwEH4","colab_type":"code","outputId":"d39ec17c-240d-4320-9010-4794ee13f07d","executionInfo":{"status":"ok","timestamp":1544615772968,"user_tz":-330,"elapsed":323573,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["vocab[200000:200005] # "],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['by vast', 'by vengeance', 'by vengeance .', 'by vera', 'by vera miles']"]},"metadata":{"tags":[]},"execution_count":37}]},{"metadata":{"id":"lwu8GMBWwEIB","colab_type":"code","colab":{}},"cell_type":"code","source":["y=trn_y\n","x=trn_term_doc.sign()\n","val_x = val_term_doc.sign()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YOQn5OD-wEIX","colab_type":"code","colab":{}},"cell_type":"code","source":["r = np.log(pr(1) / pr(0))\n","b = np.log((y==1).mean() / (y==0).mean())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9j5ArJhUwEId","colab_type":"text"},"cell_type":"markdown","source":["Here we fit regularized logistic regression where the features are the trigrams."]},{"metadata":{"id":"ebjv9Im_wEIe","colab_type":"code","outputId":"1bc96f7b-c915-4337-85a9-cd0e821665db","executionInfo":{"status":"ok","timestamp":1544615779319,"user_tz":-330,"elapsed":329890,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["m = LogisticRegression(C=0.1, dual=True)\n","m.fit(x, y);\n","\n","preds = m.predict(val_x)\n","(preds.T==val_y).mean()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["0.905"]},"metadata":{"tags":[]},"execution_count":40}]},{"metadata":{"id":"m9zeUxPewEIk","colab_type":"text"},"cell_type":"markdown","source":["Here is the $\\text{log-count ratio}$ `r`.  "]},{"metadata":{"id":"ftC5T0XKwEIl","colab_type":"code","outputId":"5517345b-1ff1-425b-c57d-87c825ba72cd","executionInfo":{"status":"ok","timestamp":1544615779323,"user_tz":-330,"elapsed":329882,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["r.shape, r"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((1, 800000),\n"," matrix([[-0.05468, -0.161  , -0.24784, ...,  1.09861, -0.69315, -0.69315]]))"]},"metadata":{"tags":[]},"execution_count":41}]},{"metadata":{"id":"2bCIwIdnwEIr","colab_type":"code","outputId":"545f30cf-f380-46a5-da4a-c6ab2a0a1bbf","executionInfo":{"status":"ok","timestamp":1544615779326,"user_tz":-330,"elapsed":329872,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["np.exp(r)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["matrix([[0.94678, 0.85129, 0.78049, ..., 3.     , 0.5    , 0.5    ]])"]},"metadata":{"tags":[]},"execution_count":42}]},{"metadata":{"id":"lgweZ3C6wEIw","colab_type":"text"},"cell_type":"markdown","source":["Here we fit regularized logistic regression where the features are the trigrams' log-count ratios."]},{"metadata":{"id":"GkJVLOL3wEIz","colab_type":"code","outputId":"e0c6ca5c-41f2-46c4-8944-bd31cfc0be8b","executionInfo":{"status":"ok","timestamp":1544615783012,"user_tz":-330,"elapsed":333543,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["# we create a random matrix and using stochastic grad desc find out optimal \n","# values and then do logistic regression\n","x_nb = x.multiply(r)\n","\n","m = LogisticRegression(dual=True, C=0.1)\n","m.fit(x_nb, y);\n","\n","val_x_nb = val_x.multiply(r)\n","preds = m.predict(val_x_nb)\n","(preds.T==val_y).mean()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["0.91768"]},"metadata":{"tags":[]},"execution_count":43}]},{"metadata":{"id":"6BYB9i_D2iwN","colab_type":"text"},"cell_type":"markdown","source":["#Lesson 11\n"]},{"metadata":{"id":"RAdCnK_TwEI2","colab_type":"text"},"cell_type":"markdown","source":["## fastai NBSVM++"]},{"metadata":{"id":"bL7R60O8wEI5","colab_type":"code","colab":{}},"cell_type":"code","source":["sl=2000"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RyQNewqGwEI-","colab_type":"code","colab":{}},"cell_type":"code","source":["# Here is how we get a model from a bag of words\n","# trn_term doc = bag of words trn_y = labels upto 2k unique words per view\n","md = TextClassifierData.from_bow(trn_term_doc, trn_y, val_term_doc, val_y, sl)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OTQirRArwEJE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"edb2e1db-f7ab-4141-ca94-812fd8c93c6e","executionInfo":{"status":"ok","timestamp":1545128965795,"user_tz":-330,"elapsed":46023,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["learner = md.dotprod_nb_learner() # fast ai generalization of model based on dot prod of naive bayes\n","learner.fit(0.02, 1, wds=1e-6, cycle_len=1)"],"execution_count":40,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"362f5d02bd6645eba897208a039db620","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["epoch      trn_loss   val_loss   <lambda>   \n","    0      0.0247     0.1191     0.91624   \n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[array([0.1191]), 0.916239999961853]"]},"metadata":{"tags":[]},"execution_count":40}]},{"metadata":{"id":"d_0INtYSwEJJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"eaa01bfc-1bb5-4c75-b38e-c5efed2be70c","executionInfo":{"status":"ok","timestamp":1545129031890,"user_tz":-330,"elapsed":74155,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["learner.fit(0.02, 2, wds=1e-6, cycle_len=1)"],"execution_count":41,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8ddfc7fe713d46ca904d1440361025cc","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Epoch', max=2, style=ProgressStyle(description_width='initial…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["epoch      trn_loss   val_loss   <lambda>   \n","    0      0.01922    0.113365   0.92156   \n","    1      0.01085    0.112204   0.92176   \n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[array([0.1122]), 0.921759999961853]"]},"metadata":{"tags":[]},"execution_count":41}]},{"metadata":{"id":"3dWbGdHOwEJP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"d0ce4c78-0306-4c6b-f372-7c0a4fe5cfbe","executionInfo":{"status":"ok","timestamp":1545129098063,"user_tz":-330,"elapsed":139926,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["learner.fit(0.02, 2, wds=1e-6, cycle_len=1)"],"execution_count":42,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de57846fb8a141f089ad503d9fd50de4","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Epoch', max=2, style=ProgressStyle(description_width='initial…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["epoch      trn_loss   val_loss   <lambda>   \n","    0      0.016589   0.111132   0.92124   \n","    1      0.010538   0.109505   0.92208   \n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[array([0.1095]), 0.9220799999809265]"]},"metadata":{"tags":[]},"execution_count":42}]},{"metadata":{"id":"026n5aNZQjU8","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","NBSVM++\n","\n","  class DotProdNB(nn.Module):\n","    def __init__(self, nf, ny, w_adj=0.4, r_adj=10):\n","        super().__init__()\n","        self.w_adj,self.r_adj = w_adj,r_adj\n","        self.w = nn.Embedding(nf+1, 1, padding_idx=0)  #nf+1 = no. of rows assume embedding = linear\n","        self.w.weight.data.uniform_(-0.1,0.1)\n","        self.r = nn.Embedding(nf+1, ny)\n","\n","    def forward(self, feat_idx, feat_cnt, sz):\n","        w = self.w(feat_idx)\n","        r = self.r(feat_idx)\n","        x = ((w+self.w_adj)*r/self.r_adj).sum(1)  # calculating activations\n","        return F.softmax(x)\n","\n","\n","\n","w=0 means we have no confidence in our answer whether right or wrong \n","w =0  emperitically does not make any sense \n","\n","regularization tries to make w = 0 and so we add additional hurdle in front of\n","reg that it wont make w = 0 as it is penalized by summation(w^2) \n","our w can be negative hence can make w+adj(w) =0 but this occurence will be \n","cause penalty of summ(w^2)\n","\n","\n","\n","'''"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cbtYfVJhQyLB","colab_type":"code","colab":{}},"cell_type":"code","source":["??DotProdNB"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6FbasluywEJZ","colab_type":"text"},"cell_type":"markdown","source":["## References"]},{"metadata":{"id":"e_2Lx_2lwEJb","colab_type":"text"},"cell_type":"markdown","source":["* Baselines and Bigrams: Simple, Good Sentiment and Topic Classification. Sida Wang and Christopher D. Manning [pdf](https://www.aclweb.org/anthology/P12-2018)"]},{"metadata":{"id":"ilY-AcHUwEJd","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}