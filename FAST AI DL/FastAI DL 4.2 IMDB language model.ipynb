{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FastAI DL 4.2 IMDB language model.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"qTpjFKRyQTcb","colab_type":"text"},"cell_type":"markdown","source":["for sentiment classification we first need a model which understands the language so we first make a model which predicts next word and then finetune it to make it do sentiment classification"]},{"metadata":{"id":"MyxdkeSqKjH9","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip install Pillow==4.1.1\n","!pip install \"fastai==0.7.0\"\n","!pip install torchtext==0.2.3\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lZ6UhQwXO3EQ","colab_type":"code","colab":{}},"cell_type":"code","source":["from fastai.learner import *\n","import torchtext\n","from torchtext import vocab,data\n","from torchtext.datasets import language_modeling\n","\n","from fastai.rnn_reg import *\n","from fastai.rnn_train import *\n","from fastai.nlp import *\n","from fastai.lm_rnn import *\n","import dill as pickle\n","from os import path\n","\n","import spacy"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-XKyu0hKRkjv","colab_type":"code","outputId":"b631daac-ef0c-4e26-add6-ddfc339e11b3","executionInfo":{"status":"ok","timestamp":1547030598274,"user_tz":-330,"elapsed":36966,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"cell_type":"code","source":["!wget http://files.fast.ai/data/aclImdb.tgz"],"execution_count":15,"outputs":[{"output_type":"stream","text":["--2019-01-09 10:43:12--  http://files.fast.ai/data/aclImdb.tgz\n","Resolving files.fast.ai (files.fast.ai)... 67.205.15.147\n","Connecting to files.fast.ai (files.fast.ai)|67.205.15.147|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 145982645 (139M) [text/plain]\n","Saving to: ‘aclImdb.tgz’\n","\n","\raclImdb.tgz           0%[                    ]       0  --.-KB/s               \raclImdb.tgz          10%[=>                  ]  13.98M  69.9MB/s               \raclImdb.tgz          26%[====>               ]  36.36M  90.9MB/s               \raclImdb.tgz          41%[=======>            ]  57.60M  96.0MB/s               \raclImdb.tgz          57%[==========>         ]  79.99M  99.9MB/s               \raclImdb.tgz          73%[=============>      ] 102.42M   102MB/s               \raclImdb.tgz          89%[================>   ] 124.09M   103MB/s               \raclImdb.tgz         100%[===================>] 139.22M   103MB/s    in 1.3s    \n","\n","2019-01-09 10:43:13 (103 MB/s) - ‘aclImdb.tgz’ saved [145982645/145982645]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"XK8cv0a9RoHS","colab_type":"code","colab":{}},"cell_type":"code","source":["!mkdir data/\n","!tar -xvzf aclImdb.tgz -C data/\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ydNfIeqfO2im","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b18ca323-e0cd-4864-e380-198a65a5a3fd","executionInfo":{"status":"ok","timestamp":1547033322568,"user_tz":-330,"elapsed":17796,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["PATH='data/aclImdb/'\n","\n","TRN_PATH = 'train/all/'\n","VAL_PATH = 'test/all/'\n","TRN = f'{PATH}{TRN_PATH}'\n","VAL = f'{PATH}{VAL_PATH}'\n","\n","%ls {PATH}"],"execution_count":27,"outputs":[{"output_type":"stream","text":["imdbEr.txt  imdb.vocab  README  \u001b[0m\u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/\n"],"name":"stdout"}]},{"metadata":{"id":"GGajl2W4QlOw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"236eb425-c029-444e-acac-a3e486dc0687","executionInfo":{"status":"ok","timestamp":1547033334967,"user_tz":-330,"elapsed":27571,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["trn_files = !ls {TRN}\n","trn_files[:10]"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['0_0.txt       1562_10.txt  24997_0.txt\\t34371_0.txt  43748_0.txt  6248_7.txt',\n"," '0_3.txt       15621_0.txt  24998_0.txt\\t3437_1.txt   43749_0.txt  6249_0.txt',\n"," '0_9.txt       1562_1.txt   24999_0.txt\\t34372_0.txt  437_4.txt\\t  6249_2.txt',\n"," '10000_0.txt   15622_0.txt  25000_0.txt\\t34373_0.txt  43750_0.txt  6249_7.txt',\n"," '10000_4.txt   15623_0.txt  2500_0.txt\\t34374_0.txt  4375_0.txt   624_9.txt',\n"," '10000_8.txt   15624_0.txt  25001_0.txt\\t34375_0.txt  43751_0.txt  6250_0.txt',\n"," '1000_0.txt    15625_0.txt  2500_1.txt\\t34376_0.txt  4375_1.txt   6250_10.txt',\n"," '10001_0.txt   15626_0.txt  25002_0.txt\\t34377_0.txt  43752_0.txt  6250_1.txt',\n"," '10001_10.txt  15627_0.txt  25003_0.txt\\t34378_0.txt  43753_0.txt  625_0.txt',\n"," '10001_4.txt   15628_0.txt  25004_0.txt\\t3437_8.txt   43754_0.txt  625_10.txt']"]},"metadata":{"tags":[]},"execution_count":28}]},{"metadata":{"id":"8Xz62kHLQtom","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a29fc612-58f0-4b1f-bfcd-ece38585228a","executionInfo":{"status":"ok","timestamp":1547033344121,"user_tz":-330,"elapsed":34581,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["# finding words in dataset\n","!find {TRN} -name '*.txt' | xargs cat| wc -w"],"execution_count":29,"outputs":[{"output_type":"stream","text":["17486581\n"],"name":"stdout"}]},{"metadata":{"id":"lzjKy2oMTLj1","colab_type":"code","colab":{}},"cell_type":"code","source":["#loading the tokenizer\n","spacy_tok = spacy.load('en')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CdLvQDC1d4VW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"3c32d8ad-8ef3-46f6-9f49-645c9da5c05f","executionInfo":{"status":"ok","timestamp":1547033354317,"user_tz":-330,"elapsed":41127,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["review = !cat {TRN}{trn_files[6]}\n","review[0]"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"I have to say when a name like Zombiegeddon and an atom bomb on the front cover I was expecting a flat out chop-socky fung-ku, but what I got instead was a comedy. So, it wasn't quite was I was expecting, but I really liked it anyway! The best scene ever was the main cop dude pulling those kids over and pulling a Bad Lieutenant on them!! I was laughing my ass off. I mean, the cops were just so bad! And when I say bad, I mean The Shield Vic Macky bad. But unlike that show I was laughing when they shot people and smoked dope.<br /><br />Felissa Rose...man, oh man. What can you say about that hottie. She was great and put those other actresses to shame. She should work more often!!!!! I also really liked the fight scene outside of the building. That was done really well. Lots of fighting and people getting their heads banged up. FUN! Last, but not least Joe Estevez and William Smith were great as the...well, I wasn't sure what they were, but they seemed to be having fun and throwing out lines. I mean, some of it didn't make sense with the rest of the flick, but who cares when you're laughing so hard! All in all the film wasn't the greatest thing since sliced bread, but I wasn't expecting that. It was a Troma flick so I figured it would totally suck. It's nice when something surprises you but not totally sucking.<br /><br />Rent it if you want to get stoned on a Friday night and laugh with your buddies. Don't rent it if you are an uptight weenie or want a zombie movie with lots of flesh eating.<br /><br />P.S. Uwe Boil was a nice touch.cat: 15625_0.txt: No such file or directory\""]},"metadata":{"tags":[]},"execution_count":31}]},{"metadata":{"id":"d-jnLdxfTPCW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"ae212296-eeea-41b2-e187-58a728fbedca","executionInfo":{"status":"ok","timestamp":1547033354325,"user_tz":-330,"elapsed":40596,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["' '.join([sent.string.strip() for sent in spacy_tok(review[0])])"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"I have to say when a name like Zombiegeddon and an atom bomb on the front cover I was expecting a flat out chop - socky fung - ku , but what I got instead was a comedy . So , it was n't quite was I was expecting , but I really liked it anyway ! The best scene ever was the main cop dude pulling those kids over and pulling a Bad Lieutenant on them ! ! I was laughing my ass off . I mean , the cops were just so bad ! And when I say bad , I mean The Shield Vic Macky bad . But unlike that show I was laughing when they shot people and smoked dope.<br /><br />Felissa Rose ... man , oh man . What can you say about that hottie . She was great and put those other actresses to shame . She should work more often ! ! ! ! ! I also really liked the fight scene outside of the building . That was done really well . Lots of fighting and people getting their heads banged up . FUN ! Last , but not least Joe Estevez and William Smith were great as the ... well , I was n't sure what they were , but they seemed to be having fun and throwing out lines . I mean , some of it did n't make sense with the rest of the flick , but who cares when you 're laughing so hard ! All in all the film was n't the greatest thing since sliced bread , but I was n't expecting that . It was a Troma flick so I figured it would totally suck . It 's nice when something surprises you but not totally sucking.<br /><br />Rent it if you want to get stoned on a Friday night and laugh with your buddies . Do n't rent it if you are an uptight weenie or want a zombie movie with lots of flesh eating.<br /><br />P.S. Uwe Boil was a nice touch.cat : 15625_0.txt : No such file or directory\""]},"metadata":{"tags":[]},"execution_count":32}]},{"metadata":{"id":"TDZT7nuNdi1L","colab_type":"code","colab":{}},"cell_type":"code","source":["# First, we create a torchtext field, which describes how to preprocess a piece of text - in this case, \n","# we tell torchtext to make everything lowercase, and tokenize it with spacy.\n","\n","TEXT = data.Field(lower = True,tokenize =\"spacy\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X1iTupeFfkpV","colab_type":"code","colab":{}},"cell_type":"code","source":["# bptt; this define how many words are processing at a time in each row of the mini-batch. \n","# More importantly, it defines how many 'layers' we will backprop through. Making this number higher will increase time \n","# and memory requirements, but will improve the model's ability to handle long sentences.\n","# Back Prop Through Time. It means how long a sentence we will stick on the GPU at once\n","\n","bs = 64\n","bptt = 70"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oZdss7sxenuO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"4552a358-d19c-4271-c82e-218d6281baab","executionInfo":{"status":"ok","timestamp":1547033656832,"user_tz":-330,"elapsed":339920,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["# create a ModelData object for language modeling by taking advantage of LanguageModelData, passing it our torchtext field object, \n","# and the paths to our training, test, and validation sets. \n","# In this case, we don't have a separate test set, so we'll just use VAL_PATH for that too.\n","\n","# min_freq=10 : In a moment, we are going to be replacing words with integers (a unique index for every word). \n","# If there are any words that occur less than 10 times, just call it unknown.\n","\n","%%time\n","FILES = dict(train = TRN_PATH,test = VAL_PATH,validation =VAL_PATH)\n","model_data = LanguageModelData.from_text_files(PATH,TEXT,**FILES,bs=bs,bptt = bptt,min_freq = 10)\n","\n","# After building our ModelData object, it automatically fills the TEXT object with a very important attribute: TEXT.vocab. \n","# This is a vocabulary, which stores which words (or tokens) have been seen in the text, \n","# and how each word will be mapped to a unique integer id. We'll need to use this information again later, so we save it.\n","\n"],"execution_count":35,"outputs":[{"output_type":"stream","text":["CPU times: user 4min 52s, sys: 9.36 s, total: 5min 1s\n","Wall time: 5min 1s\n"],"name":"stdout"}]},{"metadata":{"id":"E3xRktkshW8F","colab_type":"code","colab":{}},"cell_type":"code","source":["# save the info\n","\n","!mkdir {PATH}models/\n","pickle.dump(TEXT,open(f'{PATH}models/TEXT.pkl','wb'))\n","\n","# (Technical note: python's standard Pickle library can't handle this correctly, \n","# so at the top of this notebook we used the dill library instead and imported it as pickle)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pPPU_aF2i1Lz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f56f75d9-e409-403d-fb96-0f578d86e509","executionInfo":{"status":"ok","timestamp":1547034021708,"user_tz":-330,"elapsed":21889,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["# batches; # unique tokens in the vocab; # tokens in the training set; # sentences\n","len(model_data.trn_dl), model_data.nt, len(model_data.trn_ds), len(model_data.trn_ds[0].text)"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4583, 37392, 1, 20540756)"]},"metadata":{"tags":[]},"execution_count":37}]},{"metadata":{"id":"7zB4bw45jNq0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"59f2084e-92ff-48a0-f5e5-ca46d0d51b66","executionInfo":{"status":"ok","timestamp":1547034021716,"user_tz":-330,"elapsed":21865,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["# integer to string maps in order of frequency except 1st 3 unnamed and padding # part of torchtext\n","TEXT.vocab.itos[:12]"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is', 'in', 'it']"]},"metadata":{"tags":[]},"execution_count":38}]},{"metadata":{"id":"rXNI4SA7kZTE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a2fa41c2-aa12-4bc3-c9db-6955e27c6124","executionInfo":{"status":"ok","timestamp":1547034021728,"user_tz":-330,"elapsed":21842,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["# string to int\n","TEXT.vocab.stoi['the']"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{"tags":[]},"execution_count":39}]},{"metadata":{"id":"3Ghq-e1cky_B","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"outputId":"b2f448d6-e24e-4a41-b562-3d1d8732dd52","executionInfo":{"status":"ok","timestamp":1547034021732,"user_tz":-330,"elapsed":21813,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["# Note that in a LanguageModelData object there is only one item in each dataset: all the words of the text joined together.\n","\n","model_data.trn_ds[0].text[:12]\n"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['again',\n"," ',',\n"," 'we',\n"," 'see',\n"," 'what',\n"," 'could',\n"," 'be',\n"," 'a',\n"," 'really',\n"," 'good',\n"," 'movie',\n"," 'fail']"]},"metadata":{"tags":[]},"execution_count":40}]},{"metadata":{"id":"9grjIFXmlFHE","colab_type":"code","colab":{}},"cell_type":"code","source":["# change to integer // do mapping\n","TEXT.numericalize([model_data.trn_ds[0].text[:12]])\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gjvHBXStmk-i","colab_type":"text"},"cell_type":"markdown","source":["## BPTT and Batch Size\n","\n","we create colms of selected bs and rows of all remaining values in bs\n","\n","  [64 batches]  take 70 values at once for processing [bptt]\n","\n","---\n","\n","\n","  [64 batches]\n","\n","---\n","\n","\n","  .\n","  .\n","  upto number of words/bs times "]},{"metadata":{"id":"ctEbmanaljip","colab_type":"code","colab":{}},"cell_type":"code","source":["# get batch of data \n","next(iter(model_data.trn_dl))\n","\n","# torchtext randomly change bptt values during each epoch same as shuffling images\n","# as we cant shuffle words works same\n","\n","# 1st colm is the first 75 words of 1st segment\n","# 2nd colm is first 75 words of 2nd seg\n","\n","# in downside the matrix if flattened out for technical reasons but have same order\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TJzRQ_3XpOpE","colab_type":"text"},"cell_type":"markdown","source":["### now we have modeldata object that can feed us batches so we now make our model"]},{"metadata":{"id":"io9D0AQtngir","colab_type":"code","colab":{}},"cell_type":"code","source":["##  Generally, an embedding size for a word will be somewhere between 50 and 600.\n","\n","\n","em_sz = 200  # size of each embedding vector\n","nh = 500     # number of hidden activations per layer\n","nl = 3       # number of layers"],"execution_count":0,"outputs":[]},{"metadata":{"id":"F6coO7h--I7Y","colab_type":"code","colab":{}},"cell_type":"code","source":["# Researchers have found that large amounts of momentum (which we’ll learn about later) don’t work well with these kinds of RNN models, so we create a version of the Adam # optimizer with less momentum than its default of 0.9. Any time you are doing NLP, \n","# you should probably include this line:\n","\n","opt_fn = partial(optim.Adam, betas=(0.7, 0.99)) #optimizer function\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jPLlI1qC-2-R","colab_type":"code","colab":{}},"cell_type":"code","source":["# fastai uses a variant of the state of the art AWD LSTM Language Model developed by Stephen Merity. \n","# A key feature of this model is that it provides excellent regularization through Dropout. \n","# There is no simple way known (yet!) to find the best values of the dropout parameters below - \n","# you just have to experiment...\n","\n","# However, the other parameters (alpha, beta, and clip) shouldn't generally need tuning.\n","\n","learner = model_data.get_model(opt_fn,em_sz,nh,nl,dropouti =0.05,dropout= 0.05,dropoute=0.02,dropouth=0.05)\n","learner.reg_fn = partial(seq2seq_reg,alpha =2,beta =1) \n","learner.clip =0.3\n","\n","# if you try to build an NLP model and you are under-fitting, then decrease all these dropouts, if overfitting, \n","# then increase all these dropouts in roughly this ratio.\n","# when you look at your gradients and you multiply them by the learning rate to decide how much to update your weights by, \n","# this will not allow them be more than 0.3"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c5z7Oy8ED2Al","colab_type":"code","colab":{}},"cell_type":"code","source":["%%time\n","learner.lr_find()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0uoGQyC_D4bc","colab_type":"code","colab":{}},"cell_type":"code","source":["learner.sched.plot_lr()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Z4ZQGVrfETHu","colab_type":"code","colab":{}},"cell_type":"code","source":["# # wds = weight decay\n","## no of cycles n_cycle = 4\n","#########################################\n","#########################################\n","#original# learner.fit(3e-3, 4, wds=1e-6, cycle_len=1, cycle_mult=2)\n","learner.fit(3e-1,2,wds = 1e-2,cycle_len=1,cycle_mult=2)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hmWQ0YqfFKzf","colab_type":"code","colab":{}},"cell_type":"code","source":["learner.save_encoder('adam1_enc')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"p1KVia83FtAW","colab_type":"code","colab":{}},"cell_type":"code","source":["learner.load_encoder('adam1_enc')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l38lOuIZFt-j","colab_type":"code","colab":{}},"cell_type":"code","source":["learner.fit(3e-3, 1, wds=1e-6, cycle_len=10)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6VWD0RvPFwAv","colab_type":"code","colab":{}},"cell_type":"code","source":["# In the sentiment analysis section, we'll just need half of the language model - the encoder, so we save that part.\n","\n","learner.save_encoder('adam3_10_enc')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vrq5yWypFyH1","colab_type":"code","colab":{}},"cell_type":"code","source":["learner.load_encoder('adam3_10_enc')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8CbKj85AGOAc","colab_type":"code","colab":{}},"cell_type":"code","source":["# Language modeling accuracy is generally measured using the metric perplexity, which is simply exp() of the loss function we used.\n","# here our loss is 4.16 so\n","math.exp(4.165)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7v0v0M-qGa4M","colab_type":"code","colab":{}},"cell_type":"code","source":["pickle.dump(TEXT, open(f'{PATH}models/TEXT.pkl','wb'))"],"execution_count":0,"outputs":[]}]}