{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Fast AI Lesson 6_2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"1v6CozYsRz4-","colab_type":"text"},"cell_type":"markdown","source":["# RNN"]},{"metadata":{"id":"YPi8tmPAUP3Y","colab_type":"text"},"cell_type":"markdown","source":["We use RNN as it can also remember previous prediction whereas normal NN cant remember previous "]},{"metadata":{"id":"BAZuII3TUaNG","colab_type":"text"},"cell_type":"markdown","source":["![alt text](https://cdn-images-1.medium.com/max/800/1*vPfe01ALNgbxw8DP_4RFVw.png)\n","\n","A normal NN with 1 Hidden Layer :::::: It have Different Activation layers :::: these layers are matrices \n","\n","An activation is a number calculated by layer\n","\n","\n","A CNN with 2 fully connected layers\n","![alt text](https://cdn-images-1.medium.com/max/800/1*VEEVatttQmlWeI98vTO0iA.png)\n","\n","WE are gonna predict 3rd char from 2 input chars \n","\n","Input can be one-hot-encoded character (length of vector = # of unique characters) or a single integer and pretend it is one-hot-encoded by using an embedding layer.\n","\n","  ![alt text](https://cdn-images-1.medium.com/max/800/1*gc1z1R1d5zHkYc75iqSWtw.png)\n","  "]},{"metadata":{"id":"Gh-u9ut4OQhd","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip uninstall fastai\n","!pip uninstall Pillow\n","!pip install Pillow==4.1.1\n","!pip install \"fastai==0.7.0\"\n","!pip install torchtext==0.2.3\n","!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python\n","import cv2\n","from os import path\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","!apt update && apt install -y libsm6 libxext6\n","\n","accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n","import torch\n","!pip install image\n","\n","%matplotlib inline\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0J9qsj_1XSt0","colab_type":"code","colab":{}},"cell_type":"code","source":["%reload_ext autoreload\n","%autoreload 2\n","%matplotlib inline\n","\n","from fastai.io import *\n","from fastai.conv_learner import *\n","\n","from fastai.column_data import *"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lLJGhHtPYmz3","colab_type":"code","colab":{}},"cell_type":"code","source":["PATH='data/nietzsche/'   # we will work on nietzche text\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OTdx2RBOYvuI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"4489d977-96f3-43f2-f2ff-3d4c50e344ef","executionInfo":{"status":"ok","timestamp":1542956719730,"user_tz":-330,"elapsed":1582,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["get_data(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\", f'{PATH}nietzsche.txt')\n","text = open(f'{PATH}nietzsche.txt').read()\n","print('corpus length:', len(text))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["nietzsche.txt: 606kB [00:00, 720kB/s]                            "],"name":"stderr"},{"output_type":"stream","text":["corpus length: 600893\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"metadata":{"id":"bwvfF3AwYxej","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"294d04dc-1b8d-472e-d6d6-c7d2c8449574","executionInfo":{"status":"ok","timestamp":1542956771683,"user_tz":-330,"elapsed":2139,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["text[:400]"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'PREFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is there not ground\\nfor suspecting that all philosophers, in so far as they have been\\ndogmatists, have failed to understand women--that the terrible\\nseriousness and clumsy importunity with which they have usually paid\\ntheir addresses to Truth, have been unskilled and unseemly methods for\\nwinning a woman? Certainly she has never allowed herself '"]},"metadata":{"tags":[]},"execution_count":5}]},{"metadata":{"id":"DnwAVHXZY-B2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ccb8b056-1979-455b-b0eb-91608a2978cb","executionInfo":{"status":"ok","timestamp":1542957189377,"user_tz":-330,"elapsed":844,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["chars = sorted(list(set(text))) #set will create all the unique letters and we sort them\n","vocab_size = len(chars)+1\n","print('total chars:', vocab_size) # unique letters\n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["total chars: 85\n"],"name":"stdout"}]},{"metadata":{"id":"Y1vjXyUwZiys","colab_type":"code","colab":{}},"cell_type":"code","source":["chars.insert(0,\"\\0\") #adding padding\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"liooLVPbZrs3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"efd7e5b7-2c99-43ba-d390-69e47b688bd0","executionInfo":{"status":"ok","timestamp":1542956995874,"user_tz":-330,"elapsed":1024,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["''.join(chars[1:-6])"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\x00\\x00\\n !\"\\'(),-.0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxy'"]},"metadata":{"tags":[]},"execution_count":10}]},{"metadata":{"id":"PvtZs6cqZ1CM","colab_type":"code","colab":{}},"cell_type":"code","source":["char_indices = dict((c, i) for i, c in enumerate(chars)) # mapping every char to unique ID this will create an index i and take element from c\n","indices_char = dict((i, c) for i, c in enumerate(chars)) # mapping every ID to char\n","\n","char_indices\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"slTqD_dcaIUq","colab_type":"code","colab":{}},"cell_type":"code","source":["indices_char"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-NUWJHdcavL8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e48d1bcb-e7fc-414f-f97d-ac27d91eae0a","executionInfo":{"status":"ok","timestamp":1542957408759,"user_tz":-330,"elapsed":670,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["# creating char list of indices\n","idx = [char_indices[c] for c in text]\n","idx[:10]"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[39, 41, 28, 29, 24, 26, 28, 0, 0, 0]"]},"metadata":{"tags":[]},"execution_count":17}]},{"metadata":{"id":"UeQJ2d09bZ7F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"7c717d16-3884-4d3f-8e82-506c7e638cef","executionInfo":{"status":"ok","timestamp":1542957460855,"user_tz":-330,"elapsed":1059,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["# confirming transformation of index and char\n","''.join(indices_char[i] for i in idx[:70])"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'PREFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is there not gro'"]},"metadata":{"tags":[]},"execution_count":18}]},{"metadata":{"id":"TXtItmkabmi6","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"avWfq6wnbvoa","colab_type":"text"},"cell_type":"markdown","source":["**Question: Character based model vs. word based model**\n","\n","Generally, you want to combine character level model and word level model (e.g. for translation).\n","\n","Character level model is useful when a vocabulary contains unusual words — which word level model will just treat as “unknown”. When you see a word you have not seen before, you can use a character level model.\n","\n","There is also something in between that is called Byte Pair Encoding (BPE) which looks at n-gram of characters.\n"]},{"metadata":{"id":"xW4Rtp2rb5Ns","colab_type":"text"},"cell_type":"markdown","source":["## 3 char model\n"]},{"metadata":{"id":"9h837m12cITP","colab_type":"text"},"cell_type":"markdown","source":["### creating input"]},{"metadata":{"id":"AoYE3FtZdKdq","colab_type":"text"},"cell_type":"markdown","source":["Now let's build a model where we can pass in 3 chars and get the fourth back. For this to work, we will need to grab the 0, 1, 2, 3 position chars in a loop."]},{"metadata":{"id":"ehwwLSEvb2vC","colab_type":"code","colab":{}},"cell_type":"code","source":["# Get every nth element in indices -- our string of letters converted to indices\n","# range statement = get 0...len-3, skipping 3 chars at a time\n","\n","cs = 3 # character skip\n","c1_dat = [idx[i]   for i in range(0, len(idx)-cs, cs)]\n","c2_dat = [idx[i+1] for i in range(0, len(idx)-cs, cs)]\n","c3_dat = [idx[i+2] for i in range(0, len(idx)-cs, cs)]\n","c4_dat = [idx[i+3] for i in range(0, len(idx)-cs, cs)]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"raALFcTfdZ8d","colab_type":"code","colab":{}},"cell_type":"code","source":["# putting data into stack\n","\n","x1 = np.stack(c1_dat) \n","x2 = np.stack(c2_dat) \n","x3 = np.stack(c3_dat) \n","y = np.stack(c4_dat)  # our output"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zmIbr9evd49Q","colab_type":"code","colab":{}},"cell_type":"code","source":["n_hidden = 256"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_1u043-keBIv","colab_type":"code","colab":{}},"cell_type":"code","source":["# The number of latent factors to create (i.e. the size of the embedding matrix)\n","n_fac = 42"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tdWpO75QfxdK","colab_type":"text"},"cell_type":"markdown","source":["![alt text](https://raw.githubusercontent.com/pekoto/fast.ai/master/images/rnn5.jpg)\n","\n","\n","\n","this is what we are working on \n","same color =  same matrix"]},{"metadata":{"id":"DK8c4HKTeFHw","colab_type":"code","colab":{}},"cell_type":"code","source":["# we will have same matrix for same colored arrow\n","\n","\n","class Char3Model(nn.Module):\n","    def __init__(self, vocab_size, n_fac):\n","        super().__init__()\n","        \n","        # creating embeddings with size of vocab and outputs\n","        self.e = nn.Embedding(vocab_size, n_fac)\n","\n","        # creating linear layers l_in\n","        \n","        # The 'green arrow' from our diagram - the layer operation from input to hidden\n","        self.l_in = nn.Linear(n_fac, n_hidden)\n","\n","        # The 'orange arrow' from our diagram - the layer operation from hidden to hidden\n","        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n","        \n","        # The 'blue arrow' from our diagram - the layer operation from hidden to output\n","        self.l_out = nn.Linear(n_hidden, vocab_size)\n","        \n","    def forward(self, c1, c2, c3):\n","        in1 = F.relu(self.l_in(self.e(c1))) # each char will go through embedding->linear_layer->relu layer self.e = size 42\n","        in2 = F.relu(self.l_in(self.e(c2)))\n","        in3 = F.relu(self.l_in(self.e(c3)))\n","        \n","        h = V(torch.zeros(in1.size()).cuda()) # this is used to make *** look identical to ***2 ***3\n","        \n","        h = F.tanh(self.l_hidden(h+in1))  # ***1 it would be self.l_hidden(in1) but we made h == 0\n","        h = F.tanh(self.l_hidden(h+in2))  # ***2\n","        h = F.tanh(self.l_hidden(h+in3))  # ***3\n","        \n","        return F.log_softmax(self.l_out(h))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OxqapUG4egyR","colab_type":"code","colab":{}},"cell_type":"code","source":["md = ColumnarModelData.from_arrays('.', [-1], np.stack([x1,x2,x3], axis=1), y, bs=512)  # whatever we pass-in in np.stack we will get forward(c1,c2,c3 position)\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DamwC5yFg3-F","colab_type":"code","colab":{}},"cell_type":"code","source":["m = Char3Model(vocab_size, n_fac).cuda()  # this is std pytorch model NOT A LEARNER so using gpu"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GFSg5MiEhUFE","colab_type":"code","colab":{}},"cell_type":"code","source":["it = iter(md.trn_dl) # iterate through train data loader\n","*xs,yt = next(it) # get mini batch returns all x'es and y tensors\n","t = m(*V(xs))\n","#it is not 1hotencoded we used embedding to pretend it is\n","print(len(xs)) # c1,c2,c3"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CYkZCI4shbnL","colab_type":"code","colab":{}},"cell_type":"code","source":["# pytorch optimizer\n","opt = optim.Adam(m.parameters(), 1e-2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u3dar7k-hd0c","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"aef10b38-701c-4865-a42a-310943337496","executionInfo":{"status":"ok","timestamp":1542959013424,"user_tz":-330,"elapsed":7773,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["fit(m, md, 1, opt, F.nll_loss)"],"execution_count":32,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"693cef3ce995423fa7770e066dff5af2","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["epoch      trn_loss   val_loss   \n","    0      2.082338   0.632364  \n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[array([0.63236])]"]},"metadata":{"tags":[]},"execution_count":32}]},{"metadata":{"id":"Hni0z2kVhf8L","colab_type":"code","colab":{}},"cell_type":"code","source":["set_lrs(opt, 0.001)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QumJtmcAhiYe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"e5565741-560b-41c3-860e-764a410f03c4","executionInfo":{"status":"ok","timestamp":1542959046673,"user_tz":-330,"elapsed":7760,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["fit(m, md, 1, opt, F.nll_loss)"],"execution_count":34,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5208a8896e764c0b9dbc5f4d502f701b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["epoch      trn_loss   val_loss   \n","    0      1.815071   0.452837  \n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[array([0.45284])]"]},"metadata":{"tags":[]},"execution_count":34}]},{"metadata":{"id":"qxNgUKpwlM13","colab_type":"text"},"cell_type":"markdown","source":["### Testing the model"]},{"metadata":{"id":"GwIX2Dq9hoE7","colab_type":"code","colab":{}},"cell_type":"code","source":["# can pass in 3 chars eg. y.[space]\n","def get_next(inp):\n","    # Convert character in the input to tensor\n","    indices = T(np.array([char_indices[c] for c in inp])) #turn those into int\n","    \n","    probabilities = m(*VV(indices)) # turn into variables turn into models\n","    \n","    i = np.argmax(to_np(probabilities)) # np.argmax returns index of the maximum values. convert to np array\n","    \n","    return chars[i]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iiOiawCil6FB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"dc6dbcaf-3027-40ac-e11f-4066596c941c","executionInfo":{"status":"ok","timestamp":1542960347096,"user_tz":-330,"elapsed":2664,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["get_next('y. ')"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'I'"]},"metadata":{"tags":[]},"execution_count":44}]},{"metadata":{"id":"Es887ZYRmTFu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"56adbfd8-ae30-4693-ffd8-5fd947aeef0a","executionInfo":{"status":"ok","timestamp":1542960350037,"user_tz":-330,"elapsed":828,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["get_next('ppl')"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'e'"]},"metadata":{"tags":[]},"execution_count":45}]},{"metadata":{"id":"6TtUan8TmkGo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"67326072-ca92-4f7f-e28f-be0e15a9b7a5","executionInfo":{"status":"ok","timestamp":1542960356201,"user_tz":-330,"elapsed":840,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["get_next(' th')"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'e'"]},"metadata":{"tags":[]},"execution_count":46}]},{"metadata":{"id":"8RJSQpZRmlei","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"02e482d4-e96c-4be6-d7cd-9b5b38535918","executionInfo":{"status":"ok","timestamp":1542960357938,"user_tz":-330,"elapsed":645,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["get_next('and')\n"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' '"]},"metadata":{"tags":[]},"execution_count":47}]},{"metadata":{"id":"4ndpoGhYwKto","colab_type":"text"},"cell_type":"markdown","source":["## Creating our own model"]},{"metadata":{"id":"F1o_rskvwMhd","colab_type":"code","colab":{}},"cell_type":"code","source":["#This is the size of our unrolled RNN.\n","cs=8"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7BMtS87Ump7G","colab_type":"code","colab":{}},"cell_type":"code","source":["# For each of 0 through 7, create a list of every 8th character with that starting point. \n","# These will be the 8 inputs to our model.\n","c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(len(idx)-cs)]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8BwvNs_ZwYT5","colab_type":"code","colab":{}},"cell_type":"code","source":["# Then create a list of the next character in each of these series. This will be the labels for our model.\n","c_out_dat = [idx[j+cs] for j in range(len(idx)-cs)]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"noimpXg9wnDD","colab_type":"code","colab":{}},"cell_type":"code","source":["xs = np.stack(c_in_dat, axis=0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"96uCVWPawnd4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"460581a5-5950-4ce1-b542-81fb243b0e9b","executionInfo":{"status":"ok","timestamp":1542962974610,"user_tz":-330,"elapsed":846,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["xs.shape\n"],"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(600885, 8)"]},"metadata":{"tags":[]},"execution_count":52}]},{"metadata":{"id":"TECg0MBPwoup","colab_type":"code","colab":{}},"cell_type":"code","source":["y = np.stack(c_out_dat)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sF492jiqwp67","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"1c74ea63-4ad5-4e9d-fca0-64938f0c6690","executionInfo":{"status":"ok","timestamp":1542962985129,"user_tz":-330,"elapsed":1091,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["xs[:cs,:cs]\n"],"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[39, 41, 28, 29, 24, 26, 28,  0],\n","       [41, 28, 29, 24, 26, 28,  0,  0],\n","       [28, 29, 24, 26, 28,  0,  0,  0],\n","       [29, 24, 26, 28,  0,  0,  0, 42],\n","       [24, 26, 28,  0,  0,  0, 42, 44],\n","       [26, 28,  0,  0,  0, 42, 44, 39],\n","       [28,  0,  0,  0, 42, 44, 39, 39],\n","       [ 0,  0,  0, 42, 44, 39, 39, 38]])"]},"metadata":{"tags":[]},"execution_count":54}]},{"metadata":{"id":"hcs8QA13wrMx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f43aca30-77c0-49e8-f0a8-04cf4b6a5410","executionInfo":{"status":"ok","timestamp":1542962993859,"user_tz":-330,"elapsed":1074,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["y[:cs]\n"],"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0,  0, 42, 44, 39, 39, 38, 42])"]},"metadata":{"tags":[]},"execution_count":55}]},{"metadata":{"id":"OMnrroWwwwck","colab_type":"text"},"cell_type":"markdown","source":["###Create and train model"]},{"metadata":{"id":"_e8Xx7UEwtW0","colab_type":"code","colab":{}},"cell_type":"code","source":["val_idx = get_cv_idxs(len(idx)-cs-1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bc4wgV5zw-4a","colab_type":"code","colab":{}},"cell_type":"code","source":["md = ColumnarModelData.from_arrays('.', val_idx, xs, y, bs=512)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"btxiVAVcw_OP","colab_type":"code","colab":{}},"cell_type":"code","source":["class CharLoopModel(nn.Module):\n","    # This is an RNN!\n","    def __init__(self, vocab_size, n_fac):\n","        super().__init__()\n","        self.e = nn.Embedding(vocab_size, n_fac)\n","        self.l_in = nn.Linear(n_fac, n_hidden)\n","        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n","        self.l_out = nn.Linear(n_hidden, vocab_size)\n","        \n","    def forward(self, *cs):\n","        bs = cs[0].size(0)\n","        h = V(torch.zeros(bs, n_hidden).cuda())\n","        for c in cs:\n","            inp = F.relu(self.l_in(self.e(c)))\n","            h = F.tanh(self.l_hidden(h+inp)) # here hidden and input layers are different so we concatenate them next\n","        \n","        return F.log_softmax(self.l_out(h), dim=-1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oOI3aGs8xA2P","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","m = CharLoopModel(vocab_size, n_fac).cuda()\n","opt = optim.Adam(m.parameters(), 1e-2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UsZijiS8xCZF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"c3689e6d-53bc-4064-dca3-cff9827a75ab","executionInfo":{"status":"ok","timestamp":1542963117961,"user_tz":-330,"elapsed":32516,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["fit(m, md, 1, opt, F.nll_loss)"],"execution_count":61,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4da9614ef35c4217b61e15052273bca4","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["epoch      trn_loss   val_loss   \n","    0      1.983172   1.986563  \n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[array([1.98656])]"]},"metadata":{"tags":[]},"execution_count":61}]},{"metadata":{"id":"LA0uFatGxD_8","colab_type":"code","colab":{}},"cell_type":"code","source":["set_lrs(opt, 0.001)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"esskcT00xGf5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"d72f0a7b-23d2-4672-d063-cf3f7f38c0b4","executionInfo":{"status":"ok","timestamp":1542963149600,"user_tz":-330,"elapsed":47610,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["fit(m, md, 1, opt, F.nll_loss)"],"execution_count":63,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01dfcde732094fe6b2b559ba949c5116","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["epoch      trn_loss   val_loss   \n","    0      1.687515   1.694019  \n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[array([1.69402])]"]},"metadata":{"tags":[]},"execution_count":63}]},{"metadata":{"id":"As1aiWtsxIAx","colab_type":"code","colab":{}},"cell_type":"code","source":["# One issue with the model above is that we are adding the input and the hidden activations (h) so far. \n","# The input state represents the encoding of the characters, but h is the encoding of the series of characters so far. \n","# Since these are different types of info, we want to concatenate them, not add them.\n","class CharLoopConcatModel(nn.Module):\n","    def __init__(self, vocab_size, n_fac):\n","        super().__init__()\n","        self.e = nn.Embedding(vocab_size, n_fac)\n","        self.l_in = nn.Linear(n_fac+n_hidden, n_hidden)\n","        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n","        self.l_out = nn.Linear(n_hidden, vocab_size)\n","        \n","    def forward(self, *cs):\n","        bs = cs[0].size(0)\n","        h = V(torch.zeros(bs, n_hidden).cuda())\n","        for c in cs:\n","            inp = torch.cat((h, self.e(c)), 1) # concatenating the hidden and in layer Input is the encoding of a character, and h is an encoding of series of characters. So adding them together, we might lose information. Let’s concatenate them instead. \n","            inp = F.relu(self.l_in(inp)) # Don’t forget to change the input to match the shape (n_fac+n_hidden instead of n_fac).\n","            h = F.tanh(self.l_hidden(inp))  # replacing h with new hidden state\n","        \n","        return F.log_softmax(self.l_out(h), dim=-1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dF-12psexJqE","colab_type":"code","colab":{}},"cell_type":"code","source":["m = CharLoopConcatModel(vocab_size, n_fac).cuda()\n","opt = optim.Adam(m.parameters(), 1e-3)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4MrKEnSWxLOw","colab_type":"code","colab":{}},"cell_type":"code","source":["it = iter(md.trn_dl)\n","*xs,yt = next(it)\n","t = m(*V(xs))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nYxm-HjpxNEl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"ce8d3d06-65ab-4d41-ef3c-882a09a0201f","executionInfo":{"status":"ok","timestamp":1542963182581,"user_tz":-330,"elapsed":53725,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["fit(m, md, 1, opt, F.nll_loss)"],"execution_count":67,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e725a14803d64daea1d985a44813d96f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["epoch      trn_loss   val_loss   \n","    0      1.863278   1.841863  \n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[array([1.84186])]"]},"metadata":{"tags":[]},"execution_count":67}]},{"metadata":{"id":"xscRgeWOxOkx","colab_type":"code","colab":{}},"cell_type":"code","source":["set_lrs(opt, 1e-4)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"F_wNlmuVxQBy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"3a173100-0b45-429d-e793-75a3c061accf","executionInfo":{"status":"ok","timestamp":1542963215250,"user_tz":-330,"elapsed":74713,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["fit(m, md, 1, opt, F.nll_loss)"],"execution_count":69,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7704b7855fde49df82ffc4449e1b6763","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["epoch      trn_loss   val_loss   \n","    0      1.751296   1.760035  \n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[array([1.76004])]"]},"metadata":{"tags":[]},"execution_count":69}]},{"metadata":{"id":"hE73ChC_zJ2k","colab_type":"text"},"cell_type":"markdown","source":["###Testing model"]},{"metadata":{"id":"jeLZukXRxRai","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_next(inp):\n","    idxs = T(np.array([char_indices[c] for c in inp]))\n","    p = m(*VV(idxs))\n","    i = np.argmax(to_np(p))\n","    return chars[i]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fAoEIY4pzOe6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"864a5a3d-5fc4-4936-c606-7ede0f2ffafe","executionInfo":{"status":"ok","timestamp":1542963661981,"user_tz":-330,"elapsed":985,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["get_next('for thos')\n"],"execution_count":71,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'e'"]},"metadata":{"tags":[]},"execution_count":71}]},{"metadata":{"id":"jUhTkjNmzQfA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"be044cca-2910-4ee5-c08a-d46678f28fd0","executionInfo":{"status":"ok","timestamp":1542963670259,"user_tz":-330,"elapsed":820,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["get_next('part of ')"],"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'t'"]},"metadata":{"tags":[]},"execution_count":72}]},{"metadata":{"id":"N1_PWedBzSkB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"0449dc1f-94e0-4294-f250-59ff70056da2","executionInfo":{"status":"ok","timestamp":1542963677063,"user_tz":-330,"elapsed":900,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["get_next('queens a')"],"execution_count":73,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'n'"]},"metadata":{"tags":[]},"execution_count":73}]},{"metadata":{"id":"NR2_LxHjzXwF","colab_type":"text"},"cell_type":"markdown","source":["##RNN with pyTorch"]},{"metadata":{"id":"NGn9oa3lzUNQ","colab_type":"code","colab":{}},"cell_type":"code","source":["# we are doing same thing as above only it is wrapped in here\n","# PyTorch will write the for loop automatically for us and also the linear input layer.\n","\n","class CharRnn(nn.Module):\n","    def __init__(self, vocab_size, n_fac):\n","        super().__init__()\n","        self.e = nn.Embedding(vocab_size, n_fac)\n","        self.rnn = nn.RNN(n_fac, n_hidden)\n","        self.l_out = nn.Linear(n_hidden, vocab_size)\n","        \n","    def forward(self, *cs):\n","        bs = cs[0].size(0)\n","        h = V(torch.zeros(1, bs, n_hidden))\n","        inp = self.e(torch.stack(cs))\n","        outp,h = self.rnn(inp, h) # it returns all ellipses/ activations so we only use 1 using dim = -1\n","        # The minor difference in PyTorch is that self.rnn will append a new hidden state to a tensor instead of replacing \n","        # (in other words, it will give back all ellipses in the diagram) . We only want the final one so we do outp[-1]\n","        \n","        return F.log_softmax(self.l_out(outp[-1]), dim=-1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hl-01k7BzkvB","colab_type":"code","colab":{}},"cell_type":"code","source":["m = CharRnn(vocab_size, n_fac).cuda()\n","opt = optim.Adam(m.parameters(), 1e-3)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7bHnGmTq1Xr2","colab_type":"code","colab":{}},"cell_type":"code","source":["it = iter(md.trn_dl)\n","*xs,yt = next(it)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4jUkiW8x1l8H","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5abbd035-74fd-4a26-9b98-c5a61a2e3f5c","executionInfo":{"status":"ok","timestamp":1542964280601,"user_tz":-330,"elapsed":720,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["t = m.e(V(torch.stack(xs)))\n","t.size()"],"execution_count":78,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([8, 512, 42])"]},"metadata":{"tags":[]},"execution_count":78}]},{"metadata":{"id":"w3bLnQrs1nll","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c435a614-0643-41ce-9ce1-e7cfee72be6d","executionInfo":{"status":"ok","timestamp":1542964288557,"user_tz":-330,"elapsed":1224,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["ht = V(torch.zeros(1, 512,n_hidden))\n","outp, hn = m.rnn(t, ht)\n","outp.size(), hn.size()"],"execution_count":79,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([8, 512, 256]), torch.Size([1, 512, 256]))"]},"metadata":{"tags":[]},"execution_count":79}]},{"metadata":{"id":"AkKa22Te1pYb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6c67fb61-0008-4c72-d762-482ef83bb538","executionInfo":{"status":"ok","timestamp":1542964295934,"user_tz":-330,"elapsed":1060,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["t = m(*V(xs)); t.size()"],"execution_count":80,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([512, 85])"]},"metadata":{"tags":[]},"execution_count":80}]},{"metadata":{"id":"2WEkEBUQ1rQE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"dfeb7dd6-424c-4b01-a909-ef52a6eb9189","executionInfo":{"status":"ok","timestamp":1542964416231,"user_tz":-330,"elapsed":115422,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["\n","fit(m, md, 4, opt, F.nll_loss)"],"execution_count":81,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c81053f9186411ea26e9b3546167641","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Epoch', max=4, style=ProgressStyle(description_width='initial…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["epoch      trn_loss   val_loss   \n","    0      1.859717   1.845479  \n","    1      1.674511   1.672501  \n","    2      1.583756   1.601259  \n","    3      1.539196   1.557011  \n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[array([1.55701])]"]},"metadata":{"tags":[]},"execution_count":81}]},{"metadata":{"id":"v4ZqxG501stM","colab_type":"code","colab":{}},"cell_type":"code","source":["set_lrs(opt, 1e-4)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ULvkc3nT1uhl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"16d0d99a-e95c-4571-bf36-487b75745e91","executionInfo":{"status":"ok","timestamp":1542964473221,"user_tz":-330,"elapsed":158715,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["fit(m, md, 2, opt, F.nll_loss)"],"execution_count":83,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3ea1659f12764518ab7bc5fcb5526613","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Epoch', max=2, style=ProgressStyle(description_width='initial…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["epoch      trn_loss   val_loss   \n","    0      1.462015   1.512779  \n","    1      1.470439   1.507218  \n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[array([1.50722])]"]},"metadata":{"tags":[]},"execution_count":83}]},{"metadata":{"id":"U1-AcGZR1yly","colab_type":"text"},"cell_type":"markdown","source":["### Testing the model"]},{"metadata":{"id":"FA11poNo1wB1","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_next(inp):\n","    idxs = T(np.array([char_indices[c] for c in inp]))\n","    p = m(*VV(idxs))\n","    i = np.argmax(to_np(p))\n","    return chars[i]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"meUyzoH813ip","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5cef1115-430d-4c4e-904e-36b4a288e7a1","executionInfo":{"status":"ok","timestamp":1542964473232,"user_tz":-330,"elapsed":118556,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["get_next('for thos')"],"execution_count":85,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'e'"]},"metadata":{"tags":[]},"execution_count":85}]},{"metadata":{"id":"EFcYPmRl153h","colab_type":"code","colab":{}},"cell_type":"code","source":["def get_next_n(inp, n):\n","    res = inp\n","    for i in range(n):\n","        c = get_next(inp)\n","        res += c\n","        inp = inp[1:]+c\n","    return res"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Dmna-5tc173V","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2a9aa872-9d2d-4db0-e2c1-da1cd6079bfa","executionInfo":{"status":"ok","timestamp":1542964473238,"user_tz":-330,"elapsed":104037,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["get_next_n('for thos', 40)"],"execution_count":87,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'for those and the same the same the same the sam'"]},"metadata":{"tags":[]},"execution_count":87}]},{"metadata":{"id":"z44HdsOUAKOH","colab_type":"text"},"cell_type":"markdown","source":["##Multi-output RNN"]},{"metadata":{"id":"bBBKrN_LI8Ws","colab_type":"code","colab":{}},"cell_type":"code","source":["c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(0, len(idx)-cs-1, cs)]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"s39s95KfI_WR","colab_type":"code","colab":{}},"cell_type":"code","source":["c_out_dat = [[idx[i+j] for i in range(cs)] for j in range(1, len(idx)-cs, cs)]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IOipiojiI_hK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"0393cef5-405a-491a-bd9b-182e5fcb2a23","executionInfo":{"status":"ok","timestamp":1542969384780,"user_tz":-330,"elapsed":2720,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["xs = np.stack(c_in_dat)\n","xs.shape"],"execution_count":95,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(75111, 8)"]},"metadata":{"tags":[]},"execution_count":95}]},{"metadata":{"id":"Jl8zTKWGI_qn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"342e679c-4244-4704-dcc8-01e544c4eb83","executionInfo":{"status":"ok","timestamp":1542969391516,"user_tz":-330,"elapsed":3251,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["\n","ys = np.stack(c_out_dat)\n","ys.shape"],"execution_count":96,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(75111, 8)"]},"metadata":{"tags":[]},"execution_count":96}]},{"metadata":{"id":"T0-w7e2XJJNx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"55381ce6-3f6d-4cd0-f0a6-75665d067ddd","executionInfo":{"status":"ok","timestamp":1542969403846,"user_tz":-330,"elapsed":2759,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["xs[:cs,:cs]"],"execution_count":97,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[39, 41, 28, 29, 24, 26, 28,  0],\n","       [ 0,  0, 42, 44, 39, 39, 38, 42],\n","       [32, 37, 30,  1, 72, 60, 53, 72],\n","       [ 1, 43, 70, 73, 72, 60,  1, 61],\n","       [71,  1, 53,  1, 75, 67, 65, 53],\n","       [66,  8,  8, 75, 60, 53, 72,  1],\n","       [72, 60, 57, 66, 23,  1, 32, 71],\n","       [ 1, 72, 60, 57, 70, 57,  1, 66]])"]},"metadata":{"tags":[]},"execution_count":97}]},{"metadata":{"id":"xqjdmK-iJJLd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"7c29f826-a9e1-4904-e19c-5b5d3110402f","executionInfo":{"status":"ok","timestamp":1542969415340,"user_tz":-330,"elapsed":2838,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["ys[:cs,:cs]"],"execution_count":98,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[41, 28, 29, 24, 26, 28,  0,  0],\n","       [ 0, 42, 44, 39, 39, 38, 42, 32],\n","       [37, 30,  1, 72, 60, 53, 72,  1],\n","       [43, 70, 73, 72, 60,  1, 61, 71],\n","       [ 1, 53,  1, 75, 67, 65, 53, 66],\n","       [ 8,  8, 75, 60, 53, 72,  1, 72],\n","       [60, 57, 66, 23,  1, 32, 71,  1],\n","       [72, 60, 57, 70, 57,  1, 66, 67]])"]},"metadata":{"tags":[]},"execution_count":98}]},{"metadata":{"id":"OisJEZU7JNVG","colab_type":"text"},"cell_type":"markdown","source":["###create and train the model"]},{"metadata":{"id":"PkyKoAb9JTm4","colab_type":"code","colab":{}},"cell_type":"code","source":["val_idx = get_cv_idxs(len(xs)-cs-1)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WvDNRqe1JZix","colab_type":"code","colab":{}},"cell_type":"code","source":["md = ColumnarModelData.from_arrays('.', val_idx, xs, ys, bs=512)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Phx3RnBm19Y0","colab_type":"code","colab":{}},"cell_type":"code","source":["# we reduce redundancy and improve efficiency by removing \n","# reccuring parts of data and taking non overlapping set of\n","# characters\n","class CharSeqRnn(nn.Module):\n","    def __init__(self, vocab_size, n_fac):\n","        super().__init__()\n","        self.e = nn.Embedding(vocab_size, n_fac)\n","        self.rnn = nn.RNN(n_fac, n_hidden)\n","        self.l_out = nn.Linear(n_hidden, vocab_size)\n","        \n","    def forward(self, *cs):\n","        bs = cs[0].size(0)\n","        h = V(torch.zeros(1, bs, n_hidden))\n","        inp = self.e(torch.stack(cs))\n","        outp,h = self.rnn(inp, h)    # ***4\n","        return F.log_softmax(self.l_out(outp), dim=-1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gpd5tKpeA7rB","colab_type":"text"},"cell_type":"markdown","source":["Notice that we are no longer doing outp[-1] since we want to keep all of them. But everything else is identical. One complexity is that we want to use the negative log-likelihood loss function as before, but it expects two rank 2 tensors (two mini-batches of vectors). But here, we have rank 3 tensor: (rank =  number of directions req to describe it)\n","\n","8 characters (time steps)\n","84 probabilities\n","for 512 minibatch\n","\n","\n","\"***4\" self.rnn(inp, h) is a loop applying the same matrix multiply again and again. If that matrix multiply tends to increase the activations each time, we are effectively doing that to the power of 8 — we call this a gradient explosion. We want to make sure the initial l_hidden will not cause our activations on average to increase or decrease.\n","\n","A nice matrix that does exactly that is called identity matrix:"]},{"metadata":{"id":"I23C6gdoAwrR","colab_type":"code","colab":{}},"cell_type":"code","source":["m = CharSeqRnn(vocab_size, n_fac).cuda()\n","opt = optim.Adam(m.parameters(), 1e-3)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bMSjyoiRIofT","colab_type":"code","colab":{}},"cell_type":"code","source":["it = iter(md.trn_dl)\n","*xst,yt = next(it)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ToS6tPKVIp8R","colab_type":"code","colab":{}},"cell_type":"code","source":["# Custom Loss Function\n","\n","def nll_loss_seq(inp, targ):\n","    sl,bs,nh = inp.size()\n","    targ = targ.transpose(0,1).contiguous().view(-1)\n","    return F.nll_loss(inp.view(-1,nh), targ)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ooHpF7UgMAuq","colab_type":"text"},"cell_type":"markdown","source":["F.nll_loss is the PyTorch loss function.\n","Flatten our inputs and targets.\n","\n","Transpose the first two axes because PyTorch expects \n","1. sequence length (how many time steps), \n","2. batch size, \n","3. hidden state itself. yt.size() is 512 by 8, whereas sl, bs is 8 by 512.\n","\n","PyTorch does not generally actually shuffle the memory order when you do things like ‘transpose’, but instead it keeps some internal metadata to treat it as if it is transposed.\n","\n","When you transpose a matrix, PyTorch just updates the metadata . If you ever see an error that says “this tensor is not continuous” , add .contiguous() after it and error goes away.\n",".view is same as np.reshape. -1 indicates as long as it needs to be."]},{"metadata":{"id":"MKTeBiCyIrnU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"3d57303c-4a0d-467f-9e8c-2314a395e7c1","executionInfo":{"status":"ok","timestamp":1542969527378,"user_tz":-330,"elapsed":18428,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["fit(m, md, 4, opt, nll_loss_seq)"],"execution_count":106,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f0d12419a9a84e29ab6b1f787e63a6be","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Epoch', max=4, style=ProgressStyle(description_width='initial…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["epoch      trn_loss   val_loss   \n","    0      2.611669   2.415422  \n","    1      2.298023   2.204125  \n","    2      2.143688   2.090011  \n","    3      2.046707   2.014779  \n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[array([2.01478])]"]},"metadata":{"tags":[]},"execution_count":106}]},{"metadata":{"id":"5OZ4zImAMWaj","colab_type":"text"},"cell_type":"markdown","source":["Remember that fit(...) is the lowest level fast.ai abstraction that implements the training loop. So all the arguments are standard PyTorch things except for md which is our model data object which wraps up the test set, the training set, and the validation set."]},{"metadata":{"id":"phEwFMBpLD9Z","colab_type":"text"},"cell_type":"markdown","source":["###Identity Init"]},{"metadata":{"id":"fNg4qkyCIuLd","colab_type":"code","colab":{}},"cell_type":"code","source":["m = CharSeqRnn(vocab_size, n_fac).cuda()\n","opt = optim.Adam(m.parameters(), 1e-2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"N2y0xzzQLK6_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"outputId":"94493849-e423-4ef5-f35e-85cbbc798742","executionInfo":{"status":"ok","timestamp":1542969936280,"user_tz":-330,"elapsed":864,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["m.rnn.weight_hh_l0.data.copy_(torch.eye(n_hidden)) # We can overwrite the randomly initialized hidden-hidden weight with an identity matrix:\n"],"execution_count":108,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    1     0     0  ...      0     0     0\n","    0     1     0  ...      0     0     0\n","    0     0     1  ...      0     0     0\n","       ...          ⋱          ...       \n","    0     0     0  ...      1     0     0\n","    0     0     0  ...      0     1     0\n","    0     0     0  ...      0     0     1\n","[torch.cuda.FloatTensor of size 256x256 (GPU 0)]"]},"metadata":{"tags":[]},"execution_count":108}]},{"metadata":{"id":"yiqstlW5LMUy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"090e0156-61ba-408e-c7f7-c08ada255346","executionInfo":{"status":"ok","timestamp":1542969958143,"user_tz":-330,"elapsed":16320,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["fit(m, md, 4, opt, nll_loss_seq)\n"],"execution_count":109,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b8a96ae649d54987a382e8de465f255b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Epoch', max=4, style=ProgressStyle(description_width='initial…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["epoch      trn_loss   val_loss   \n","    0      2.424316   2.265299  \n","    1      2.156431   2.096609  \n","    2      2.038416   2.011768  \n","    3      1.976292   1.959522  \n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[array([1.95952])]"]},"metadata":{"tags":[]},"execution_count":109}]},{"metadata":{"id":"eU-5GzwwLN56","colab_type":"code","colab":{}},"cell_type":"code","source":["set_lrs(opt, 1e-3)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6EndxPXALP7q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"0aa0552d-1455-4170-dd1c-649384bd4c71","executionInfo":{"status":"ok","timestamp":1542969974422,"user_tz":-330,"elapsed":19080,"user":{"displayName":"Himanshu Kashyap","photoUrl":"https://lh6.googleusercontent.com/-iKW3ClKVQtg/AAAAAAAAAAI/AAAAAAAAAXA/acgkyzmJyyw/s64/photo.jpg","userId":"05163588411483717003"}}},"cell_type":"code","source":["fit(m, md, 4, opt, nll_loss_seq)\n"],"execution_count":111,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f7f4e2b8adb497db4ebd7b7ad68a140","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Epoch', max=4, style=ProgressStyle(description_width='initial…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["epoch      trn_loss   val_loss   \n","    0      1.881031   1.891031  \n","    1      1.872315   1.883545  \n","    2      1.859521   1.876229  \n","    3      1.853916   1.871369  \n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[array([1.87137])]"]},"metadata":{"tags":[]},"execution_count":111}]},{"metadata":{"id":"O21F5tI8LRKe","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}